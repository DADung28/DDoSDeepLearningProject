{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/.pyenv/versions/3.10.5/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-07-26 14:14:13,588\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-07-26 14:14:16,693\tINFO worker.py:1621 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Import library for load data\n",
    "import argparse\n",
    "import ray\n",
    "ray.init(num_cpus=20)\n",
    "import modin.pandas as pd\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from category_encoders import OrdinalEncoder\n",
    "from numpy import genfromtxt\n",
    "import sys\n",
    "import csv\n",
    "# Import deeplearning library\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms \n",
    "from torch.utils.data import Dataset\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import numpy as np \n",
    "from CustomFunction import *\n",
    "from model_define import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_PATH = os.getcwd()\n",
    "validated_dir = WORK_PATH  + '/validated_balanced'\n",
    "os.makedirs(validated_dir, exist_ok=True) \n",
    "#----------------PARSE PARAMETER-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------PARAMETERS-------------\n",
      "Current work path: /home/CICIDS\n",
      "Dataset PATH: ./dataset_ddos/CICIDS2017.csv\n",
      "Dataset type: CICIDS2017_anomaly\n",
      "Training on GPU: 0\n",
      "Batch size: 1000\n",
      "Trained model PATH: /home/CICIDS/trained_model/CICIDS2017_anomaly/CNN_0/3epoch.pth\n"
     ]
    }
   ],
   "source": [
    "#----------------PARSE PARAMETER-----------------\n",
    "parser = argparse.ArgumentParser(description='This program will take trained model and validate on dataset]')\n",
    "# parser.add_argument: Add parameter to program\n",
    "WORK_PATH = os.getcwd()\n",
    "parser.add_argument('--device', type = int, help='Define GPU device for training', default=0) # Required parameter\n",
    "parser.add_argument('--ver', help='Traning model version name (Scheduled, NonScheduled, Test) (default: Scheduled)', default='Scheduled') \n",
    "parser.add_argument('--dataset', help=f'Training dataset path (default: {WORK_PATH}/dataset/TrainCIC2019.csv)', default=WORK_PATH + '/dataset/TrainCIC2019.csv') # Required parameter\n",
    "parser.add_argument('--model', help='Trained model name (ANN, ANND, DNN, DNND)(default: ANN)', default='ANN') \n",
    "parser.add_argument('--epoch', help='Trained model epoch_number (default: 100)', default='100') \n",
    "parser.add_argument('--bs', type = int, help='Batch size for traning (Default: 10000)', default= 10000)\n",
    "parser.add_argument('--trained_type', help='Type of trained model (\"train\" for TrainCIC2019.csv or \"test\" for TestCIC2019.csv, default: train)', default='train') \n",
    "parser.add_argument('--test_type', help='Type of dataset (\"train\" for TrainCIC2019.csv or \"test\" for TestCIC2019.csv, default: train)', default='train') \n",
    "#args = parser.parse_args()\n",
    "args = parser.parse_args(args=['--epoch','3', '--dataset', './dataset_ddos/CICIDS2017.csv', '--model', 'CNN', '--ver', '0', '--trained_type', 'CICIDS2017_anomaly', '--test_type', 'CICIDS2017_anomaly', '--bs', '1000'])\n",
    "\n",
    "#----------------Define variable based of args----------\n",
    "GPU_NUM = args.device\n",
    "dataset_PATH = args.dataset\n",
    "model_type = args.model\n",
    "ver = args.ver\n",
    "epoch_num = args.epoch\n",
    "batch_size = args.bs\n",
    "test_type = args.test_type\n",
    "trained_type = args.trained_type\n",
    "trained_model_PATH =  WORK_PATH + '/trained_model' + '/' + trained_type + '/' + model_type + '_' + ver + '/' + epoch_num + 'epoch.pth'\n",
    "# Make folder for validate data file\n",
    "print('-------------PARAMETERS-------------')\n",
    "print('Current work path:',WORK_PATH)\n",
    "print('Dataset PATH:', dataset_PATH)\n",
    "print('Dataset type:', test_type)\n",
    "print('Training on GPU:',GPU_NUM)\n",
    "print('Batch size:', batch_size)\n",
    "print('Trained model PATH:', trained_model_PATH)\n",
    "\n",
    "# Define CUDA device \n",
    "device = torch.device(GPU_NUM if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------ANALYZE DATASET-------------\n",
      "Loading dataset, please wait .....\n",
      "DONE!\n",
      "-------------------------\n",
      "Start fixing columns name\n",
      "DONE!\n",
      "-------------------------\n",
      "Start dropping NaN and Infinity contained row\n",
      "DONE!\n",
      "-------------------------\n",
      "Start encoding flow labels to number\n",
      "DONE!\n",
      "-------------------------\n",
      "Dropping ('Unnamed: 0', 'Flow ID','Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Timestamp', 'SimillarHTTP')\n",
      "No columns to drop\n",
      "DONE!\n",
      "-------------------------\n",
      "Change data into numpy\n",
      "DONE!\n",
      "-------------------------\n",
      "Split numpy data to data and label\n",
      "(1446598, 78)\n",
      "77\n",
      "DONE!\n",
      "-------------------------\n",
      "Normalizing data into range [0,1] for training\n",
      "DONE!\n",
      "-------------------------\n",
      "Split train, validate and test dataset: 90:5:5\n",
      "DONE!\n",
      "Train shape: (1157278, 77), Validate shape: (144660, 77), Test shape: (144660, 77)\n",
      "-------------------------\n",
      "Analyze data took: 5.60s\n"
     ]
    }
   ],
   "source": [
    "print('-------------ANALYZE DATASET-------------')\n",
    "analyze_start =  time.time()\n",
    "#Load dataset\n",
    "print(\"Loading dataset, please wait .....\")\n",
    "train = pd.read_csv(dataset_PATH)\n",
    "#test = pd.read_csv('/home/jun/CICIDS2019/TestCIC2019.csv')\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Fix columns name\n",
    "print(\"Start fixing columns name\")\n",
    "train = fix_columns_name(train)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "\n",
    "# Drop NaN and inf\n",
    "print(\"Start dropping NaN and Infinity contained row\")\n",
    "train = drop_NaN(train)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Encoding train dataset label\n",
    "print(\"Start encoding flow labels to number\")\n",
    "train = encoding_labels(test_type, train)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "output_size = len(set(train['Label']))\n",
    "# Drop num numberic columns ('Flow ID','Source IP','Destination IP', 'Timestamp', 'SimillarHTTP')\n",
    "print(\"Dropping ('Unnamed: 0', 'Flow ID','Source IP', 'Source Port', 'Destination IP', 'Destination Port', 'Timestamp', 'SimillarHTTP')\")\n",
    "try:\n",
    "    train = drop_non_numberic_columns(train)\n",
    "except:\n",
    "    print(\"No columns to drop\")\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "input_size = len(train.columns)-1\n",
    "output_size = len(set(train['Label']))\n",
    "# Change data to numpy\n",
    "print(\"Change data into numpy\")\n",
    "train = train.to_numpy()\n",
    "train = np.asfarray(train)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Split dataset to data and label\n",
    "print(\"Split numpy data to data and label\")\n",
    "print(train.shape)\n",
    "print(input_size)\n",
    "train_day_X = train[:, :input_size]\n",
    "train_day_y = train[:, input_size]\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Normalize data to range (0, 1)\n",
    "print(\"Normalizing data into range [0,1] for training\")\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_day_X)\n",
    "train_day_X = scaler.transform(train_day_X)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Slit training day data to train (90%), val (5%) and test (5%)\n",
    "print(\"Split train, validate and test dataset: 90:5:5\")\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(train_day_X, train_day_y, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.5, random_state=42) \n",
    "print(\"DONE!\")\n",
    "print('Train shape: {}, Validate shape: {}, Test shape: {}'.format(X_train.shape, X_val.shape, X_test.shape))\n",
    "print(\"-------------------------\")\n",
    "analyze_end =  time.time()\n",
    "print('Analyze data took: {:.2f}s'.format(analyze_end - analyze_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = np.concatenate((X_test,y_test.reshape(y_test.shape[0],1)),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144660, 78)\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign: 5000\n",
      "DDoS: 5000\n",
      "new_test_dataset (10000, 78)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "new_test_dataset = []\n",
    "count_0 = 0\n",
    "count_1 = 0\n",
    "for flow in test_dataset:\n",
    "    if flow[77] == 0 and count_0 < 5000:\n",
    "        count_0 += 1\n",
    "        new_test_dataset.append(flow) \n",
    "    if flow[77] == 1 and count_1 < 5000:\n",
    "        count_1 += 1\n",
    "        new_test_dataset.append(flow) \n",
    "new_test_dataset = np.array(new_test_dataset)\n",
    "\n",
    "print('Benign:',count_0)\n",
    "print('DDoS:',count_1)\n",
    "print('new_test_dataset',new_test_dataset.shape)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign: 10000\n",
      "DDoS: 10000\n"
     ]
    }
   ],
   "source": [
    "for flow in new_test_dataset:\n",
    "    if flow[77] == 0:\n",
    "        count_0 += 1\n",
    "    if flow[77] == 1:\n",
    "        count_1 += 1\n",
    "print('Benign:',count_0)\n",
    "print('DDoS:',count_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = new_test_dataset[:, :input_size]\n",
    "y_test = new_test_dataset[:, input_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------TRAINING-------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------TRAINING-------------')\n",
    "Test_dataset = MyDataset(X_test.astype(np.float32), y_test.astype(np.float32))\n",
    "# Create a data loader for batch training\n",
    "test_loader = torch.utils.data.DataLoader(Test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (linear1): Linear(in_features=77, out_features=4800, bias=True)\n",
      "  (conv1): Conv2d(3, 40, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(40, 200, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=9800, out_features=5000, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=5000, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create Model to load\n",
    "# Define Model  \n",
    "if model_type == 'ANN':\n",
    "    model = ANN(input_size, output_size)\n",
    "elif model_type == 'ANND':\n",
    "    model = ANND(input_size, output_size)\n",
    "elif model_type == 'DNN':\n",
    "    model = DNN(input_size, output_size)\n",
    "elif model_type == 'DNND':\n",
    "    model = DNND(input_size, output_size)\n",
    "elif model_type == 'CNN':\n",
    "    model = CNN(input_size, output_size)\n",
    "elif model_type == 'CNN2':\n",
    "    model = CNN2(input_size, output_size)\n",
    "elif model_type == 'CNN3':\n",
    "    model = CNN3(input_size, output_size)\n",
    "\n",
    "model.load_state_dict(torch.load(trained_model_PATH, map_location=device))\n",
    "# Push model to GPU\n",
    "model.to(device)\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (linear1): Linear(in_features=77, out_features=4800, bias=True)\n",
      "  (conv1): Conv2d(3, 40, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu1): ReLU()\n",
      "  (maxpool1): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(40, 200, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (relu2): ReLU()\n",
      "  (maxpool2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (fc1): Linear(in_features=9800, out_features=5000, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (fc2): Linear(in_features=5000, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(trained_model_PATH, map_location=device))\n",
    "# Push model to GPU\n",
    "model.to(device)\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(model)\n",
    "\n",
    "def get_test_loss_acc(net, criterion, data_loader):\n",
    "    \"\"\"A simple function that iterates over `data_loader` to calculate the overall loss\"\"\"\n",
    "    testing_loss = []\n",
    "    testing_acc = []\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    # Define confusion matrix\n",
    "    c = torch.zeros((output_size, output_size))\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            # Get the data to GPU (if available)\n",
    "            inputs, labels = inputs.to(device), labels.to(device, dtype = torch.long) # Change to data type long to not rise error\n",
    "            outputs = net(inputs)\n",
    "            # Calculate the loss for this batch\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Add the loss of this batch to the list\n",
    "            testing_loss.append(loss.item())\n",
    "            _, predicted = torch.max(outputs, 1) # Get prediction on batch (shape: batch_size)\n",
    "            # Add up confusion matrix in each batch to confusion matrix\n",
    "            c += ConfusionMatrix(labels, predicted, output_size)\n",
    "            batch_correct_predicted = (predicted == labels).sum().item() # Get number of true prediction on batch\n",
    "            testing_acc.append(batch_correct_predicted/batch_size*100) # Append prediction accuracy of this batch\n",
    "    # Calculate the average loss and accuracy\n",
    "    avg_loss = np.mean(testing_loss) \n",
    "    avg_acc = np.mean(testing_acc) \n",
    "    return {'loss': avg_loss, 'acc': avg_acc, 'confusion_matrix':c}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = get_test_loss_acc(model, criterion, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "avg_test_loss = result['loss']\n",
    "avg_test_acc = result['acc']\n",
    "c_matrix = result['confusion_matrix'] \n",
    "validate_result = Validate(c_matrix)\n",
    "actual = validate_result['actual']\n",
    "total_acc = validate_result['accuracy'] \n",
    "precision = validate_result['precision'] \n",
    "precision_dict = {}\n",
    "for i,p in enumerate(precision):\n",
    "    precision_dict.update({i:np.around(float(p),decimals=3)})\n",
    "    \n",
    "recall = validate_result['recall'] \n",
    "recall_dict = {}\n",
    "for i,r in enumerate(recall):\n",
    "    recall_dict.update({i:np.around(float(r),decimals=3)})\n",
    "f1_score = validate_result['f1_score'] \n",
    "f1_score_dict = {}\n",
    "for i,f in enumerate(f1_score):\n",
    "    f1_score_dict.update({i:np.around(float(f),decimals=3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test average accuracy: 95.22%, Test average loss: 0.106\n",
      "Confusion matrix:\n",
      " [[4895  105]\n",
      " [ 373 4627]]\n",
      "Benign : DDoS = 5000 : 5000 = 50.00 : 50.00\n",
      "Total accuracy: 95.22%\n",
      "Precision: {0: 0.929, 1: 0.978}\n",
      "Recall: {0: 0.979, 1: 0.925}\n",
      "f1_score: {0: 0.953, 1: 0.951}\n"
     ]
    }
   ],
   "source": [
    "txt_file_PATH = validated_dir + '/' + trained_type.split('_')[0] + '_' + test_type.split('_')[0] + '.txt' \n",
    "f = open(txt_file_PATH, 'w')\n",
    "\n",
    "f.write('Test average accuracy: {:.2f}%, Test average loss: {:.3f}'.format(avg_test_acc, avg_test_loss))\n",
    "f.write('\\n')\n",
    "print('Test average accuracy: {:.2f}%, Test average loss: {:.3f}'.format(avg_test_acc, avg_test_loss))\n",
    "f.write(f'Confusion matrix:\\n {c_matrix.type(torch.int).numpy()}')\n",
    "f.write('\\n')\n",
    "print('Confusion matrix:\\n', c_matrix.type(torch.int).numpy())\n",
    "f.write(f'Benign : DDoS = {int(actual[0])} : {int(actual[1])} = {(actual[0]/torch.sum(actual)*100).item():.2f} : {(actual[1]/torch.sum(actual)*100).item():.2f}')\n",
    "f.write('\\n')\n",
    "print(f'Benign : DDoS = {int(actual[0])} : {int(actual[1])} = {(actual[0]/torch.sum(actual)*100).item():.2f} : {(actual[1]/torch.sum(actual)*100).item():.2f}')\n",
    "f.write(f'Total accuracy: {total_acc*100:.2f}%')\n",
    "f.write('\\n')\n",
    "print(f'Total accuracy: {total_acc*100:.2f}%')\n",
    "f.write(f'Precision: {precision_dict}')\n",
    "f.write('\\n')\n",
    "print(f'Precision: {precision_dict}')\n",
    "f.write(f'Recall: {recall_dict}')\n",
    "f.write('\\n')\n",
    "print(f'Recall: {recall_dict}')\n",
    "f.write(f'f1_score: {f1_score_dict}')\n",
    "f.write('\\n')\n",
    "print(f'f1_score: {f1_score_dict}')\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
