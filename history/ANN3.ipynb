{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-18 01:29:52,848\tINFO worker.py:1636 -- Started a local Ray instance.\n"
     ]
    }
   ],
   "source": [
    "# Import library for load data\n",
    "import ray\n",
    "ray.init(num_cpus=8)\n",
    "import modin.pandas as pd\n",
    "#import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from category_encoders import OrdinalEncoder\n",
    "from numpy import genfromtxt\n",
    "import sys\n",
    "import csv\n",
    "# Import deeplearning library\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms \n",
    "from torch.utils.data import Dataset\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The columns name of dataset is contain space, this function will strip it out (data = fix_columns_name(data))\n",
    "def fix_columns_name(data):\n",
    "    data.columns = [column_name.strip() for column_name in data.columns]\n",
    "    return data\n",
    "\n",
    "# Drop all string data in dataset (data = drop_non_numberic_columns(data))\n",
    "def drop_non_numberic_columns(data):\n",
    "    data.drop(['Unnamed: 0', 'Flow ID','Source IP','Destination IP', 'Timestamp', 'SimillarHTTP'], inplace=True, axis=1)\n",
    "    return data\n",
    "\n",
    "#Create a copy of pandas frame with dropped NaN and Inf value (data = drop_NaN(data))\n",
    "def drop_NaN(data):\n",
    "    dropped = data.copy(deep=True)\n",
    "    dropped.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    dropped.dropna(inplace=True)\n",
    "    return dropped\n",
    "\n",
    "# Return a dictionary of available labels and it number from dataset 'Label' column (labels_dict = available_lable(data))\n",
    "# Input: dataset\n",
    "# Output: \n",
    "# Traning dataset: {'Syn': 0, 'DrDoS_SNMP': 1, 'DrDoS_UDP': 2, 'DrDoS_DNS': 3, 'TFTP': 4, 'DrDoS_LDAP': 5, 'UDP-lag': 6, 'DrDoS_NTP': 7, 'DrDoS_MSSQL': 8, 'DrDoS_NetBIOS': 9, 'BENIGN': 10, 'WebDDoS': 11, 'DrDoS_SSDP': 12}\n",
    "# Testing dataset {'Syn': 0, 'NetBIOS': 1, 'UDPLag': 2, 'LDAP': 3, 'MSSQL': 4, 'BENIGN': 5, 'Portmap': 6, 'UDP': 7} \n",
    "def available_label(data):\n",
    "    data_labels = set(data['Label'])\n",
    "    labels_dict = {}\n",
    "    for i, label in enumerate(data_labels):\n",
    "        labels_dict.update({label:i})\n",
    "    return labels_dict\n",
    "\n",
    "# Caculate flow labels number and return dictionary of {labels : count}\n",
    "# train_flow_labels_count = flow_labels_count(data)\n",
    "def flows_labels_count(data):\n",
    "    labels = {}\n",
    "    for flow_label in data['Label']:\n",
    "        if flow_label in labels.keys():\n",
    "            labels[flow_label] += 1\n",
    "        else:\n",
    "            labels[flow_label] = 0\n",
    "    return labels\n",
    "\n",
    "# Encoding labels to assigned number for learning\n",
    "# data = encoding_lables('train', data)\n",
    "def encoding_labels(dataset_type, data):\n",
    "    if dataset_type == 'train':\n",
    "        labels_num = {'DrDoS_DNS': 0, 'DrDoS_UDP': 1, 'TFTP': 2, 'Syn': 3, 'DrDoS_MSSQL': 4, 'UDP-lag': 5, 'DrDoS_NetBIOS': 6, 'DrDoS_NTP': 7, 'WebDDoS': 8, 'DrDoS_SSDP': 9, 'BENIGN': 10, 'DrDoS_SNMP': 11, 'DrDoS_LDAP': 12}\n",
    "    elif dataset_type == 'test':\n",
    "        labels_num = {'Syn': 3, 'NetBIOS': 6, 'UDPLag': 5, 'LDAP': 12, 'MSSQL': 4, 'BENIGN': 10, 'Portmap': 13, 'UDP': 1} \n",
    "    for label, number in labels_num.items():\n",
    "        data['Label'] = data['Label'].replace(label, number)\n",
    "    return data\n",
    "\n",
    "# Ploting data\n",
    "def data_plot(data):\n",
    "    data_flow_labels_count = flows_labels_count(data)\n",
    "    for flow_label, flow_number in data_flow_labels_count.items():\n",
    "        print('Number of {} flow is: {}/{} ({:.2f})'.format(flow_label, flow_number, sum(data_flow_labels_count.values()), flow_number/sum(data_flow_labels_count.values())*100))\n",
    "    #Create Flow Number based on label Figure\n",
    "    data_flows_figure_num = plt.figure(figsize = (20, 5))\n",
    "    # creating the bar plot\n",
    "    plt.bar(data_flow_labels_count.keys(), data_flow_labels_count.values(), color ='red',\n",
    "            width = 0.5)\n",
    "    plt.xlabel(\"Flows Label\")\n",
    "    plt.ylabel(\"Number of FLows\")\n",
    "    plt.title(\"Flows label and number\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------ANALYZE DATASET-------------\n",
      "Loading dataset, please wait .....\n",
      "DONE!\n",
      "-------------------------\n",
      "Start fixing columns name\n",
      "DONE!\n",
      "-------------------------\n",
      "Start dropping NaN and Infinity contained row\n",
      "DONE!\n",
      "-------------------------\n",
      "Start encoding flow labels to number\n",
      "DONE!\n",
      "-------------------------\n",
      "Dropping ('Flow ID','Source IP','Destination IP', 'Timestamp', 'SimillarHTTP')\n",
      "DONE!\n",
      "-------------------------\n",
      "Shuffling data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 2929 MiB, 3 objects, write throughput 741 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 5757 MiB, 11 objects, write throughput 803 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "-------------------------\n",
      "Change data into numpy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 10512 MiB, 23 objects, write throughput 940 MiB/s.\n",
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 16907 MiB, 31 objects, write throughput 953 MiB/s.\n",
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 73747 MiB, 119 objects, write throughput 2380 MiB/s.\n",
      "\u001b[2m\u001b[36m(raylet)\u001b[0m Spilled 77436 MiB, 122 objects, write throughput 2413 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE!\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "print('-------------ANALYZE DATASET-------------')\n",
    "#Load dataset\n",
    "print(\"Loading dataset, please wait .....\")\n",
    "train = pd.read_csv('/home/jun/CICIDS2019/TrainCIC2019.csv')\n",
    "#test = pd.read_csv('/home/jun/CICIDS2019/TestCIC2019.csv')\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Fix columns name\n",
    "print(\"Start fixing columns name\")\n",
    "train = fix_columns_name(train)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Drop NaN and inf\n",
    "print(\"Start dropping NaN and Infinity contained row\")\n",
    "train = drop_NaN(train)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Encoding train dataset label\n",
    "print(\"Start encoding flow labels to number\")\n",
    "train = encoding_labels('train', train)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Drop num numberic columns ('Flow ID','Source IP','Destination IP', 'Timestamp', 'SimillarHTTP')\n",
    "print(\"Dropping ('Flow ID','Source IP','Destination IP', 'Timestamp', 'SimillarHTTP')\")\n",
    "train = drop_non_numberic_columns(train)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "#Shuffle data for training \n",
    "print(\"Shuffling data\")\n",
    "train = train.sample(frac=1)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# Change data to numpy\n",
    "print(\"Change data into numpy\")\n",
    "train = train.to_numpy()\n",
    "train = np.asfarray(train)\n",
    "print(\"DONE!\")\n",
    "print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset to data and label\n",
    "train_day_X = train[:, :81]\n",
    "train_day_y = train[:, 81]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data to range (0, 1)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_day_X)\n",
    "train_normalized = scaler.transform(train_day_X)\n",
    "\n",
    "# Slit training day data to train (80%), val (0.0002) and test (0.1998)\n",
    "X_train, X_val_test, y_train, y_val_test = train_test_split(train_day_X, train_day_y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=0.99, random_state=42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Deeplearning parameters\n",
    "batch_size = 10000\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CUDA device \n",
    "device = torch.device(3 if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (l1): Linear(in_features=81, out_features=128, bias=True)\n",
       "  (l2): Linear(in_features=128, out_features=256, bias=True)\n",
       "  (l3): Linear(in_features=256, out_features=512, bias=True)\n",
       "  (l4): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (l5): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (l6): Linear(in_features=128, out_features=13, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.25, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Custom dataset class\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.from_numpy(data)\n",
    "        self.labels = torch.from_numpy(labels)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "Train_dataset = MyDataset(X_train.astype(np.float32), y_train.astype(np.float32))\n",
    "Val_dataset = MyDataset(X_val.astype(np.float32), y_val.astype(np.float32))\n",
    "Test_dataset = MyDataset(X_test.astype(np.float32), y_test.astype(np.float32))\n",
    "\n",
    "# Create a data loader for batch training\n",
    "train_loader = torch.utils.data.DataLoader(Train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(Val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(Test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Neural Model\n",
    "# ANN (inputs_size = 81, 128, 256, 512, 256, 128, 13)\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(81, 128)\n",
    "        self.l2 = nn.Linear(128, 256)\n",
    "        self.l3 = nn.Linear(256, 512)\n",
    "        self.l4 = nn.Linear(512, 256)\n",
    "        self.l5 = nn.Linear(256, 128)\n",
    "        self.l6 = nn.Linear(128, 13)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.l4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.l5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.l6(out)\n",
    "        return out\n",
    "    \n",
    "def get_test_loss_acc(net, criterion, data_loader):\n",
    "    \"\"\"A simple function that iterates over `data_loader` to calculate the overall loss\"\"\"\n",
    "    testing_loss = []\n",
    "    testing_acc = []\n",
    "    avg_loss = 0\n",
    "    avg_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            # get the data to GPU (if available)\n",
    "            inputs, labels = inputs.to(device), labels.to(device, dtype = torch.long) # Change to data type long to not rise error\n",
    "            outputs = net(inputs)\n",
    "            # calculate the loss for this batch\n",
    "            loss = criterion(outputs, labels)\n",
    "            # add the loss of this batch to the list\n",
    "            testing_loss.append(loss.item())\n",
    "            _, predicted = torch.max(outputs, 1) # Get prediction on batch (shape: batch_size)\n",
    "            batch_correct_predicted = (predicted == labels).sum().item() # Get number of true prediction on batch\n",
    "            testing_acc.append(batch_correct_predicted/batch_size*100) # Append prediction accuracy of this batch\n",
    "    avg_loss = np.mean(testing_loss) \n",
    "    avg_acc = np.mean(testing_acc) \n",
    "    \n",
    "    # calculate the average loss\n",
    "    return {'loss': avg_loss, 'acc': avg_acc}\n",
    "\n",
    "# Define model, put into GPU\n",
    "model = NeuralNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model, put into GPU\n",
    "model = NeuralNet()\n",
    "model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "writer = SummaryWriter()\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "\n",
    "# Create csv file to plot data\n",
    "f = open('/home/jun/CICIDS2019/csv/ANN3.csv', 'w')\n",
    "writer = csv.writer(f)\n",
    "header = ['10000 batch', 'train_loss', 'train_acc', 'test_loss', 'test_acc']\n",
    "writer.writerow(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100, step [1/3409], train_loss = 1186813.500, train_acc = 5.410, test_loss = 1159653.925, test_acc = 19.215\n",
      "epoch 1/100, step [101/3409], train_loss = 49924.117, train_acc = 28.823, test_loss = 200.588, test_acc = 31.943\n",
      "epoch 1/100, step [201/3409], train_loss = 67.622, train_acc = 39.773, test_loss = 9.874, test_acc = 40.527\n",
      "epoch 1/100, step [301/3409], train_loss = 9.109, train_acc = 42.000, test_loss = 4.649, test_acc = 41.075\n",
      "epoch 1/100, step [401/3409], train_loss = 7.262, train_acc = 42.313, test_loss = 3.410, test_acc = 41.253\n",
      "epoch 1/100, step [501/3409], train_loss = 4.323, train_acc = 42.437, test_loss = 2.245, test_acc = 41.423\n",
      "epoch 1/100, step [601/3409], train_loss = 3.098, train_acc = 42.522, test_loss = 2.224, test_acc = 41.337\n",
      "epoch 1/100, step [701/3409], train_loss = 3.050, train_acc = 42.692, test_loss = 3.547, test_acc = 41.487\n",
      "epoch 1/100, step [801/3409], train_loss = 2.718, train_acc = 42.824, test_loss = 1.891, test_acc = 41.601\n",
      "epoch 1/100, step [901/3409], train_loss = 2.141, train_acc = 42.879, test_loss = 1.820, test_acc = 41.473\n",
      "epoch 1/100, step [1001/3409], train_loss = 1.946, train_acc = 42.947, test_loss = 2.435, test_acc = 41.763\n",
      "epoch 1/100, step [1101/3409], train_loss = 2.554, train_acc = 43.053, test_loss = 1.920, test_acc = 41.857\n",
      "epoch 1/100, step [1201/3409], train_loss = 2.126, train_acc = 43.018, test_loss = 2.283, test_acc = 41.581\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m running_acc\u001b[39m.\u001b[39mclear()\n\u001b[1;32m     40\u001b[0m \u001b[39m# Adding batch loss to total loss\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39m# Calculate average test loss on all val dataset\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m avg_test_loss \u001b[39m=\u001b[39m get_test_loss_acc(model, criterion, val_loader)[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     43\u001b[0m \u001b[39m# Calculate average train accuracy on all val dataset\u001b[39;00m\n\u001b[1;32m     44\u001b[0m avg_test_acc \u001b[39m=\u001b[39m get_test_loss_acc(model, criterion, val_loader)[\u001b[39m'\u001b[39m\u001b[39macc\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[10], line 61\u001b[0m, in \u001b[0;36mget_test_loss_acc\u001b[0;34m(net, criterion, data_loader)\u001b[0m\n\u001b[1;32m     59\u001b[0m avg_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     60\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m data_loader:\n\u001b[1;32m     62\u001b[0m         inputs, labels \u001b[39m=\u001b[39m data\n\u001b[1;32m     63\u001b[0m         \u001b[39m# get the data to GPU (if available)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_data()\n\u001b[1;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_fetcher\u001b[39m.\u001b[39mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/py311/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "train_loss_d, train_acc_d, test_loss_d, test_acc_d = [], [], [], []\n",
    "running_loss = []\n",
    "running_acc = []\n",
    "n_total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    # Total loss on all training sample\n",
    "    total_loss = []\n",
    "    # Total accuracy on all training sample\n",
    "    total_acc = []\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        # Batch size: 10000 \n",
    "        # 1000000, 81\n",
    "        labels = labels.to(device, dtype = torch.long) # Change labels to long data type for cross entropy\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # forward \n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Batch loss\n",
    "        running_loss.append(loss.item())\n",
    "\n",
    "        # Batch accuracy calculate\n",
    "        _, predicted = torch.max(outputs, 1) # Get prediction on batch (shape: batch_size)\n",
    "        batch_correct_predicted = (predicted == labels).sum().item() # Get number of true prediction on batch\n",
    "        running_acc.append(batch_correct_predicted/batch_size*100) # Append prediction accuracy \n",
    "        if i % 100 == 0:\n",
    "            # Calculate average train loss on 10000 batch\n",
    "            avg_train_loss = np.mean(running_loss)\n",
    "            running_loss.clear()\n",
    "            # Calculate average train accuracy on 10000 batch\n",
    "            avg_train_acc = np.mean(running_acc)\n",
    "            running_acc.clear()\n",
    "            # Adding batch loss to total loss\n",
    "            # Calculate average test loss on all val dataset\n",
    "            avg_test_loss = get_test_loss_acc(model, criterion, val_loader)['loss']\n",
    "            # Calculate average train accuracy on all val dataset\n",
    "            avg_test_acc = get_test_loss_acc(model, criterion, val_loader)['acc']\n",
    "            total_loss.append(avg_train_loss)\n",
    "            # Adding batch accuracy to total accuracy\n",
    "            total_acc.append(avg_train_acc)\n",
    "            # Adding data to plot after training\n",
    "            train_loss_d.append(avg_train_loss) \n",
    "            train_acc_d.append(avg_train_acc) \n",
    "            test_loss_d.append(avg_test_loss) \n",
    "            test_acc_d.append(avg_test_acc) \n",
    "            print(f'epoch {epoch + 1}/{num_epochs}, step [{i+1}/{n_total_step}], train_loss = {avg_train_loss:.3f}, train_acc = {avg_train_acc:.3f}, test_loss = {avg_test_loss:.3f}, test_acc = {avg_test_acc:.3f}')\n",
    "            # Write plot data to csv file\n",
    "            plot_data = [i+1,avg_train_loss,avg_train_acc,avg_test_loss,avg_test_acc]\n",
    "            writer.writerow(plot_data)\n",
    "    scheduler.step()\n",
    "    print('Epoch: [{}/{}], train_Loss: {:.3f}, train_accuracy: {:.3f}'.format(epoch + 1, num_epochs, np.mean(total_loss), np.mean(total_acc)))\n",
    "    PATH = '/home/jun/CICIDS2019/saved_model/ANN3' + str(epoch+1) + 'epoch.pth'\n",
    "    torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size for training\n",
    "batch_size = 1000000\n",
    "\n",
    "# Create a data loader for batch training\n",
    "train_loader = torch.utils.data.DataLoader(Train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(Test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/100, step [1/35], loss = 1616964.250, acc = 7.861\n",
      "epoch 1/100, step [2/35], loss = 18479442.000, acc = 10.9936\n",
      "epoch 1/100, step [3/35], loss = 2396613.750, acc = 23.03\n",
      "epoch 1/100, step [4/35], loss = 392116.938, acc = 31.4328\n",
      "epoch 1/100, step [5/35], loss = 118942.055, acc = 13.0602\n",
      "epoch 1/100, step [6/35], loss = 46043.098, acc = 14.7407\n",
      "epoch 1/100, step [7/35], loss = 12960.219, acc = 19.0364\n",
      "epoch 1/100, step [8/35], loss = 939.276, acc = 9.8412\n",
      "epoch 1/100, step [9/35], loss = 80.231, acc = 7.4739\n",
      "epoch 1/100, step [10/35], loss = 15.849, acc = 12.1159\n",
      "epoch 1/100, step [11/35], loss = 9.274, acc = 30.7457\n",
      "epoch 1/100, step [12/35], loss = 5.992, acc = 38.9699\n",
      "epoch 1/100, step [13/35], loss = 4.974, acc = 40.0546\n",
      "epoch 1/100, step [14/35], loss = 4.049, acc = 40.054899999999996\n",
      "epoch 1/100, step [15/35], loss = 3.805, acc = 40.0996\n",
      "epoch 1/100, step [16/35], loss = 2.786, acc = 40.048\n",
      "epoch 1/100, step [17/35], loss = 2.693, acc = 40.0648\n",
      "epoch 1/100, step [18/35], loss = 2.925, acc = 40.1809\n"
     ]
    }
   ],
   "source": [
    "# Neural Model\n",
    "class NeuralNet0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.l1 = nn.Linear(81, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(128, 256)\n",
    "        self.l3 = nn.Linear(256, 512)\n",
    "        self.l4 = nn.Linear(512, 13)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.l1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.l2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.l4(out)\n",
    "model0 = NeuralNet()\n",
    "num_epochs = 100\n",
    "learning_rate = 0.01\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model0.parameters(), lr = learning_rate)\n",
    "#writer.add_graph(model, samples.reshape(-1, 28*28))\n",
    "#writer.add_graph(model, examples)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.5)\n",
    "# Training loop\n",
    "n_total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    # Total loss on all training sample\n",
    "    total_loss = 0\n",
    "    # Total accuracy on all training sample\n",
    "    total_accuracy = 0\n",
    "    for i, (data, labels) in enumerate(train_loader):\n",
    "        # Batch size: 1000000\n",
    "        # Total batch: 341 \n",
    "        # 1000000, 81\n",
    "        labels = labels.to(device, dtype = torch.long) # Change labels to long data type for cross entropy\n",
    "        data = data.to(device)\n",
    "        \n",
    "        # forward \n",
    "        outputs = model0(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Batch loss\n",
    "        batch_loss = loss.item()\n",
    "        \n",
    "        # Batch accuracy calculate\n",
    "        _, predicted = torch.max(outputs, 1) # Get prediction on batch (shape: batch_size)\n",
    "        batch_correct_predicted = (predicted == labels).sum().item() # Get number of true prediction on batch\n",
    "        batch_accuracy = batch_correct_predicted/batch_size*100 # Get prediction accuracy \n",
    "        \n",
    "        # Adding batch loss to total loss\n",
    "        total_loss += batch_loss\n",
    "        # Adding batch accuracy to total accuracy\n",
    "        total_accuracy += batch_accuracy\n",
    "        print(f'epoch {epoch + 1}/{num_epochs}, step [{i+1}/{n_total_step}], loss = {batch_loss:.3f}, acc = {batch_accuracy}')\n",
    "    scheduler.step()\n",
    "    print('Epoch: [{}/{}], Loss: {:.3f}, Accuracy: {:.3f}'.format(epoch + 1, num_epochs, total_loss/n_total_step, total_accuracy/n_total_step))\n",
    "    PATH = '/home/jun/CICIDS2019/saved_model/lr0.0005model' + str(epoch+1) + 'epoch.pth'\n",
    "    torch.save(model.state_dict(), PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
